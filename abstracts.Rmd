---
title: "Abstracts"
description: |
  Below are the abstracts for the talks, in the order of the presentation. 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{css, echo = FALSE}
d-article p {
  text-align: justify;
}
```

## Oral Presentations

### Block 1: Theoretical Statistics

#### Ichiro Hashimoto
**Universality of Benign Overfitting in Binary Linear Classification**\
The practical success of deep learning has led to the discovery of several surprising phenomena. One of these phenomena, that has spurred intense theoretical research, is ``benign overfitting'': deep neural networks seem to generalize well in the over-parametrized regime even though the networks show a perfect fit to noisy training data. It is now known that benign overfitting also occurs in various classical statistical models. For linear maximum margin classifiers, benign overfitting has been established theoretically in a class of mixture models with very strong assumptions on the covariate distribution. However, even in this simple setting, many questions remain open. For instance, most of the existing literature focuses on the noiseless case where all true class labels are observed without errors, whereas the more interesting noisy case remains poorly understood. We provide a comprehensive study of benign overfitting for linear maximum margin classifiers. We discover a phase transition in test error bounds for the noisy model which was previously unknown and provide some geometric intuition behind it. We further considerably relax the required covariate assumptions in both, the noisy and noiseless case. Our results demonstrate that benign overfitting of maximum margin classifiers holds in a much wider range of scenarios than was previously known and provide new insights into the underlying mechanisms.

#### Bingqing Li
**Regression based EM Algorithm**\
High-dimensional clustering is a crucial yet challenging problem in statistics and machine learning. We propose a novel method that addresses these challenges by capturing the low-rank structure of the data. Our approach employs a Gaussian mixture model on the low-rank signal, enabling efficient and effective multiclass clustering. The method is computationally fast, leveraging Principal Component Analysis (PCA) as an initial transformation to project the data into a lower-dimensional space.This is followed by iterative updates of the projection matrix for clustering in the reduced subspace.Through comprehensive simulations across diverse scenarios, our method consistently outperforms existing techniques on high-dimensional datasets, providing a robust framework for high-dimensional clustering with enhanced efficiency. Inspired by Fisher Discriminant Analysis, we also propose a novel 2D visualization technique. This method projects high-dimensional data into a 2D space using information from the estimated labels, offering a distinctive visualization approach in unsupervised learning scenarios.


### Block 2: Applied Statistics

#### Mandy Yao
**Quantifying Uncertainty in Air Pollution Machine Learning Models**\
Given the ongoing climate crisis and increase in extreme weather events, it is now more important than ever to study the role of the environment on human health. Given the abundance and complexity of environmental data from multiple sources, machine learning (ML) methods have risen in popularity over more traditional statistical methods to explore, understand, and capture spatial and temporal trends. Yet, many ML methods have limited or no ability to quantify uncertainty, which is often needed to make insightful interpretations about predictions. We examine a popular ML method, Extreme Gradient Boosting (XGBoost), and show how a modified quantile regression can be incorporated to construct point-wise prediction intervals of specific quantiles, while allowing XGBoost to perform well by finding solutions rapidly using optimal gradient descent rates. We then compare our method to another modified quantile regression method (which uses an arctan pinball loss function), and to the implementation of quantile regression for XGBoost in the xgboost python package, by predicting particulate matter air quality exposures in California that capture wildfire events.

#### Lin Yu
**Causal Variance Decompositions for Measuring Health Inequalities**\
Racial disparities in healthcare are well-documented, but what drives them? Is it differences in hospital quality, unequal access, or hospitals treating patients differently based on race? To answer these questions, we propose a new causal decomposition method that partitions the observed variation in care received (e.g., assignment of a treatment) into five components: 1) variation due to race, 2) variation in hospital quality, 3) effect modification (differential treatment within hospitals), 4) differential access to hospitals, and 5) residual. Our method overcomes the limitations of traditional effect modification approaches by enabling overall evaluation of variation of multi-categorical variables, rather than relying on pairwise comparisons, thus allowing for assessment of the validity of the quality indicator for hospital performance comparison. Additionally, our method enhances existing variance decomposition methods by introducing two new causal estimands that interpret variance from effect modification and differential access. We propose both parametric (generalized linear models, generalized linear mixed-effect models) and nonparametric (Random Forest and XGboost) estimators for the causal components. Although initially conceptualized to address racial disparities, our method is generalizable and can be applied to study other disparities in healthcare and other domains. Simulation results show the proposed estimators capture the true causal estimands well except for small sample bias.

#### Tianyi Pan
**Estimating Associations Between Cumulative Exposure and Health via Generalized Distributed Lag Non-Linear Models using Penalized Splines**\
Quantifying associations between short-term exposure to ambient air pollution and health outcomes is an important public health priority. Historically, studies have restricted attention to single-day exposures or (equally weighted) average exposure over several days. Adaptive cumulative exposure distributed lag non-linear models (ACE-DLNMs), in contrast, quantify associations between health outcomes and cumulative exposure that is specified in a data-adaptive way. While the ACE-DLNM framework is highly interpretable, it is limited to continuous outcomes and does not scale well to large datasets. Motivated by a large analysis of daily pollution and circulatory and respiratory hospitalizations in Canada between 2001 and 2018, we propose a generalized ACE-DLNM incorporating penalized splines, and we propose an efficient estimation strategy based on profile likelihood and Laplace approximate marginal likelihood with Newton-type methods. Our proposed method improves upon existing approaches in three ways: (1) it applies to general response types, including over-dispersed counts; (2) estimation is computationally efficient and readily applies to large datasets; and (3) it treats the exposure process continuously with respect to time. In application to the motivating analysis, the proposed method respects the discrete responses and reduces uncertainty in estimated associations compared to generalized additive models with fixed exposures.


### Block 3: Mathematical Finance and Actuarial Science

#### Hassan Abdelrahman
**Simplifying Complexities in IBNR Claims Count Estimation With A Bayesian GLM Approach**\
Estimating the count of incurred but not reported (IBNR) claims is a fundamental challenge in loss reserving. The Chain Ladder method, a widely used macro-level approach, relies on aggregated claims data and provides a simple framework for reserve estimation. However, it can be inaccurate in many cases as it does not leverage detailed claims information and may introduce biases under certain conditions. To address these limitations, micro-level models have been developed to incorporate individual claim data, capturing claim occurrence and reporting dynamics more effectively. Recent literature has shown that these models outperform the Chain Ladder method in predictive accuracy. 
    
Despite their better performance, micro-level models remain largely unused in practice due to their computational complexity and various modeling challenges. In this paper, we propose a Bayesian framework that builds on the Chain Ladder method while incorporating key micro-level elements, bridging the gap between these two approaches. Through case studies, we demonstrate that our framework not only outperforms the classical Chain Ladder method but also surpasses micro-level models adopted in recent literature, offering a practical and scalable alternative for IBNR claim count estimation.

#### Brandon Tam
**Dimension Reduction of Distributionally Robust Optimization Problems**\
We study distributionally robust optimization (DRO) problems with uncertainty sets consisting of high dimensional random vectors that are close in the multivariate Wasserstein distance to a reference random vector. We give conditions under which the images of these sets under scalar-valued aggregation functions are equal to or contained in uncertainty sets of univariate random variables defined via a univariate Wasserstein distance. This allows to rewrite or bound high-dimensional DRO problems with simpler DRO problems over the space of univariate random variables. We generalize the results to uncertainty sets defined via the Bregman-Wasserstein divergence and the max-sliced Wasserstein and Bregman-Wasserstein divergence. The max-sliced divergences allow us to jointly model distributional uncertainty around the reference random vector and uncertainty in the aggregation function. Finally, we derive explicit bounds for worst-case risk measures that belong to the class of signed Choquet integrals.


### Block 4: Graphical Models

#### Philip Choi
**Inference for graphical models for extremes**\
Extreme value theory is the study of the tail region of a distribution, where empirical estimation is often impossible due to the lack of extreme data. In real-life situations, such as quantifying the risks of floods or financial crises, we are sometimes interested in the dependence of extremes. For example, how does a high water level at one measuring site relate to water levels at other measuring sites? In high-dimensional settings, tail dependence can become arbitrarily complex. Hence, sparsity is required to obtain interpretable models. The classical notion of graphical models fails for extremes, but an appropriate notion of conditional independence for extremes was proposed in 2020 by Engelke and Hitz. Since then, several data-driven estimators for graphical models for extremes have been developed. In this talk, I will discuss my current progress in building a framework for inference for these estimators. In particular, I will focus on Hüsler–Reiss graphical models for extremes, which can be thought of as analogous to Gaussian graphical models.


#### Morris Greenberg
**Restricted Search Space Graph MCMC via Birth-Death Processes**\
Inferring a directed acyclic graph (DAG) given data is computationally challenging due to graphs existing in a discrete search space that grows super-exponentially with the number of nodes. A promising class of MCMC methods for graph inference addresses scalability by first restricting the search space to a subset of edges (where partial scores can be calculated in advance), and thereafter incrementally expanding the space until a stopping criterion is met.
    
In this work, we estimate lower and upper bounds on the error introduced from current methods that operate on restricted spaces instead of the full space. Building on this, we propose a novel restricted-search MCMC method that reduces these errors. Our method is an adaptive algorithm which allows for either expansion or contraction of the search space throughout the chain. This is determined by a birth-death process, which we derive by choosing birth and death rates which are informed by our error bounds. Additionally, we improve upon the computational costs of previous restricted-search methods by including block-matrix operations in expansion steps and memoization in contraction steps. 
    
We present extensive simulations that characterize the performance and computational efficiency of our algorithm, contrast this with existing methods, and consider applications in the field of imaging proteomics.




